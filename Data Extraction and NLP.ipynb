{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Output Data Structure1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...             NaN   \n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...             NaN   \n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...             NaN   \n",
       "3      40  https://insights.blackcoffer.com/will-machine-...             NaN   \n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla...             NaN   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0             NaN             NaN                 NaN                  NaN   \n",
       "1             NaN             NaN                 NaN                  NaN   \n",
       "2             NaN             NaN                 NaN                  NaN   \n",
       "3             NaN             NaN                 NaN                  NaN   \n",
       "4             NaN             NaN                 NaN                  NaN   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                          NaN        NaN                               NaN   \n",
       "1                          NaN        NaN                               NaN   \n",
       "2                          NaN        NaN                               NaN   \n",
       "3                          NaN        NaN                               NaN   \n",
       "4                          NaN        NaN                               NaN   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                 NaN         NaN                NaN                NaN   \n",
       "1                 NaN         NaN                NaN                NaN   \n",
       "2                 NaN         NaN                NaN                NaN   \n",
       "3                 NaN         NaN                NaN                NaN   \n",
       "4                 NaN         NaN                NaN                NaN   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
       "       'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
       "       'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
       "       'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
       "       'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def sentimental_analysis():\n",
    "    read_files = glob.glob(\"StopWords\\\\*.txt\")\n",
    "    with open(\"stopwords.txt\", \"wb\") as outfile:\n",
    "        for f in read_files:\n",
    "            with open(f, \"rb\") as infile:\n",
    "                outfile.write(infile.read())\n",
    "            \n",
    "    # creating stop words list\n",
    "    stop_word_list = open('stopwords.txt').read().split('\\n')\n",
    "    stop_word_list.sort()\n",
    "    positive_words = {}\n",
    "    negative_words = {}\n",
    "\n",
    "    with open(\"MasterDictionary/negative-words.txt\", \"r\") as f:\n",
    "        negative_words = {word.strip(): 1 for word in f.readlines() if word.strip() not in stop_word_list}\n",
    "    \n",
    "    with open(\"MasterDictionary/positive-words.txt\", \"r\") as f:\n",
    "        positive_words = {word.strip(): 1 for word in f.readlines() if word.strip() not in stop_word_list}\n",
    "    \n",
    "    text_file = open(f'{a1}.txt', \"r\")\n",
    "    data = text_file.read()\n",
    "    text_file.close()\n",
    "    #print(data)\n",
    "    words = word_tokenize(data)\n",
    "    #print(words)\n",
    "    \n",
    "    # Count the number of positive words in the text\n",
    "    num_positive = sum([1 for token in words if token.lower() in positive_words])\n",
    "\n",
    "    # Count the number of negative words in the text\n",
    "    num_negative = sum([1 for token in words if token.lower() in negative_words])\n",
    "\n",
    "    # Count the polarity score in the text\n",
    "    polarity = (num_positive - num_negative)/ ((num_positive + num_negative) + 0.000001)\n",
    "    \n",
    "    # Count the number of non-stopwords in the text\n",
    "    num_non_stopwords = sum([1 for token in words if token.lower() not in stop_word_list])\n",
    "\n",
    "    # Count the subjective score in the text\n",
    "    # subjectivity = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "    subjectivity = (num_positive + num_negative)/ ((num_non_stopwords) + 0.000001)\n",
    "    \n",
    "    return num_positive, num_negative, polarity, subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from textstat import syllable_count\n",
    "\n",
    "def analysis_of_readability():\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(data)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(data)\n",
    "\n",
    "    # Calculate the average sentence length\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "\n",
    "    # Tokenize the text into words using nltk and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for word in tokenizer.tokenize(data) if word.lower() not in stop_words]\n",
    "\n",
    "    # Find the complex words\n",
    "    complex_words = set(word for word in words if syllable_count(word) > 2)\n",
    "\n",
    "    # Calculate the percentage of complex words\n",
    "    num_complex_words = sum([1 for word in words if word.lower() in complex_words])\n",
    "    pct_complex_words = (num_complex_words / num_words) * 100\n",
    "\n",
    "    # Calculate the Fog Index\n",
    "    fog_index = 0.4 * (avg_sentence_length + pct_complex_words)\n",
    "    \n",
    "    return avg_sentence_length, pct_complex_words, fog_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def average_number_of_words_per_sentence():\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(data)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(data)\n",
    "\n",
    "    # Calculate the average sentence length\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "    # Calculate the average number of words per sentence\n",
    "    avg_num_words_per_sentence = num_words / num_sentences\n",
    "    \n",
    "    return avg_num_words_per_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from textstat import syllable_count\n",
    "\n",
    "def complex_word_count():\n",
    "    # Tokenize the text into words using nltk and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for word in tokenizer.tokenize(data) if word.lower() not in stop_words]\n",
    "\n",
    "    # Find the complex words\n",
    "    complex_words = set(word for word in words if syllable_count(word) > 2)\n",
    "\n",
    "    # Count the complex word count from the text\n",
    "    num_complex_words = sum([1 for word in words if word.lower() in complex_words])\n",
    "    \n",
    "    return num_complex_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "def words_count():\n",
    "    words = nltk.word_tokenize(data)\n",
    "\n",
    "    # Remove stop words and punctuation marks from the words\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    cleaned_words = [word.lower() for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "\n",
    "    # Count the remaining cleaned words\n",
    "    num_cleaned_words = len(cleaned_words)\n",
    "    \n",
    "    return num_cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textstat import syllable_count\n",
    "\n",
    "def syllable_count_per_word():\n",
    "    # Tokenize the text into words using nltk and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for word in tokenizer.tokenize(data) if word.lower() not in stop_words]\n",
    "\n",
    "    # function to count the number of syllables in a word\n",
    "    def count_syllables(word):\n",
    "        # Remove trailing \"es\" or \"ed\"\n",
    "        if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "            word = word[:-2]\n",
    "        # Count the number of vowels\n",
    "        count = 0\n",
    "        for char in word:\n",
    "            if char in \"aeiouy\":\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    # Count the syllables in each word\n",
    "    syllable_counts = len([count_syllables(word) for word in words])\n",
    "    \n",
    "    return syllable_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def personal_pronouns():\n",
    "    # Define the regular expression pattern to match the personal pronouns\n",
    "    pattern = r'\\b(I|we|my|ours|us)\\b'\n",
    "\n",
    "    # Use the regular expression pattern to find the counts of the personal pronouns\n",
    "    matches = re.findall(pattern, data)\n",
    "\n",
    "    # Remove any matches that are the country name \"US\"\n",
    "    num_pronouns = len([match for match in matches if match != 'US'])\n",
    "    \n",
    "    return num_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def average_word_length(data):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(data)\n",
    "\n",
    "    # Remove any punctuation from the words\n",
    "    words = [word.strip(string.punctuation) for word in words]\n",
    "\n",
    "    # Calculate the total number of characters in each word\n",
    "    total_chars = sum(len(word) for word in words)\n",
    "\n",
    "    # Calculate the total number of words\n",
    "    total_words = len(words)\n",
    "\n",
    "    # Calculate the average word length\n",
    "    avg_word_length = total_chars / total_words\n",
    "    \n",
    "    return float(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error opening URL 44 https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: HTTP Error 404: Not Found\n",
      "Error opening URL 51 https://insights.blackcoffer.com/future-of-work-how-ai-has-entered-the-workplace/: 'NoneType' object has no attribute 'text'\n",
      "Error opening URL 57 https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: HTTP Error 404: Not Found\n",
      "Error opening URL 84 https://insights.blackcoffer.com/how-small-business-can-survive-the-coronavirus-crisis/: 'charmap' codec can't encode character '\\u20b9' in position 41: character maps to <undefined>\n",
      "Error opening URL 87 https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-tourism-aviation-industries/: 'charmap' codec can't encode character '\\u20b9' in position 212: character maps to <undefined>\n",
      "Error opening URL 91 https://insights.blackcoffer.com/human-rights-outlook/: 'NoneType' object has no attribute 'text'\n",
      "Error opening URL 92 https://insights.blackcoffer.com/how-voice-search-makes-your-business-a-successful-business/: 'NoneType' object has no attribute 'text'\n",
      "Error opening URL 96 https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets-2/: 'charmap' codec can't encode character '\\u20b9' in position 191: character maps to <undefined>\n",
      "Error opening URL 100 https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-3/: 'NoneType' object has no attribute 'text'\n",
      "Error opening URL 107 https://insights.blackcoffer.com/how-covid-19-is-impacting-payment-preferences/: 'NoneType' object has no attribute 'text'\n",
      "Error opening URL 108 https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/: 'NoneType' object has no attribute 'text'\n",
      "Error opening URL 112 https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-3/: 'NoneType' object has no attribute 'text'\n",
      "Error opening URL 117 https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-2/: 'charmap' codec can't encode character '\\u2248' in position 188: character maps to <undefined>\n",
      "Error opening URL 120 https://insights.blackcoffer.com/why-scams-like-nirav-modi-happen-with-indian-banks/: 'charmap' codec can't encode character '\\u20b9' in position 91: character maps to <undefined>\n",
      "Error opening URL 140 https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy/: 'charmap' codec can't encode character '\\u20b9' in position 116: character maps to <undefined>\n",
      "Error opening URL 142 https://insights.blackcoffer.com/impact-of-covid-19coronavirus-on-the-indian-economy/: 'charmap' codec can't encode character '\\u20b9' in position 149: character maps to <undefined>\n",
      "Error opening URL 144 https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/: HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "for i in range(114):\n",
    "    try:\n",
    "        url = urlopen(df['URL'][i])\n",
    "        page = url.read()\n",
    "        html = bs(page.decode('utf-8', 'ignore'), 'html.parser')\n",
    "        title = html.find(\"h1\" ,{\"class\":\"entry-title\"}).text\n",
    "        content = html.find(\"div\",{\"class\":\"td-post-content\"})\n",
    "        para = content.find_all(\"p\")\n",
    "        a1 = df['URL_ID'][i]\n",
    "        f = open(f'{a1}.txt',\"w\")\n",
    "        f.write(title)\n",
    "        f.write('\\n')\n",
    "        for _ in range(len(para)):\n",
    "            f.write(para[_].text)\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "    \n",
    "        text_file = open(f'{a1}.txt', \"r\")\n",
    "        data = text_file.read()\n",
    "        text_file.close()\n",
    "    \n",
    "        num_positive, num_negative, polarity, subjectivity = sentimental_analysis()\n",
    "        df.loc[i,'POSITIVE SCORE'] = num_positive\n",
    "        df.loc[i,'NEGATIVE SCORE'] = num_negative\n",
    "        df.loc[i,'POLARITY SCORE'] = polarity\n",
    "        df.loc[i,'SUBJECTIVITY SCORE'] = subjectivity\n",
    "    \n",
    "        avg_sentence_length, pct_complex_words, fog_index = analysis_of_readability()\n",
    "        df.loc[i,'AVG SENTENCE LENGTH'] = avg_sentence_length\n",
    "        df.loc[i,'PERCENTAGE OF COMPLEX WORDS'] = pct_complex_words\n",
    "        df.loc[i,'FOG INDEX'] = fog_index\n",
    "    \n",
    "        avg_num_words_per_sentence = average_number_of_words_per_sentence()\n",
    "        df.loc[i,'AVG NUMBER OF WORDS PER SENTENCE'] = avg_num_words_per_sentence\n",
    "    \n",
    "        num_complex_words = complex_word_count()\n",
    "        df.loc[i,'COMPLEX WORD COUNT'] = num_complex_words\n",
    "    \n",
    "        num_cleaned_words = words_count()\n",
    "        df.loc[i,'WORD COUNT'] = num_cleaned_words\n",
    "    \n",
    "        syllable_counts = syllable_count_per_word()\n",
    "        df.loc[i,'SYLLABLE PER WORD'] = syllable_counts\n",
    "    \n",
    "        num_pronouns = personal_pronouns()\n",
    "        df.loc[i,'PERSONAL PRONOUNS'] = num_pronouns\n",
    "    \n",
    "        avg_word_length = average_word_length(data)\n",
    "        df.loc[i,'AVG WORD LENGTH'] = avg_word_length\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error opening URL {df['URL_ID'][i]} {df['URL'][i]}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114 entries, 0 to 113\n",
      "Data columns (total 15 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   URL_ID                            114 non-null    int64  \n",
      " 1   URL                               114 non-null    object \n",
      " 2   POSITIVE SCORE                    97 non-null     float64\n",
      " 3   NEGATIVE SCORE                    97 non-null     float64\n",
      " 4   POLARITY SCORE                    97 non-null     float64\n",
      " 5   SUBJECTIVITY SCORE                97 non-null     float64\n",
      " 6   AVG SENTENCE LENGTH               97 non-null     float64\n",
      " 7   PERCENTAGE OF COMPLEX WORDS       97 non-null     float64\n",
      " 8   FOG INDEX                         97 non-null     float64\n",
      " 9   AVG NUMBER OF WORDS PER SENTENCE  97 non-null     float64\n",
      " 10  COMPLEX WORD COUNT                97 non-null     float64\n",
      " 11  WORD COUNT                        97 non-null     float64\n",
      " 12  SYLLABLE PER WORD                 97 non-null     float64\n",
      " 13  PERSONAL PRONOUNS                 97 non-null     float64\n",
      " 14  AVG WORD LENGTH                   97 non-null     float64\n",
      "dtypes: float64(13), int64(1), object(1)\n",
      "memory usage: 13.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.081535</td>\n",
       "      <td>26.640000</td>\n",
       "      <td>18.068068</td>\n",
       "      <td>17.883227</td>\n",
       "      <td>26.640000</td>\n",
       "      <td>361.0</td>\n",
       "      <td>1153.0</td>\n",
       "      <td>1163.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.065065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.115023</td>\n",
       "      <td>20.487500</td>\n",
       "      <td>10.982306</td>\n",
       "      <td>12.587923</td>\n",
       "      <td>20.487500</td>\n",
       "      <td>180.0</td>\n",
       "      <td>771.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.176937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.262136</td>\n",
       "      <td>0.093297</td>\n",
       "      <td>22.470588</td>\n",
       "      <td>17.591623</td>\n",
       "      <td>16.024885</td>\n",
       "      <td>22.470588</td>\n",
       "      <td>336.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.795288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.101336</td>\n",
       "      <td>19.293478</td>\n",
       "      <td>12.619718</td>\n",
       "      <td>12.765279</td>\n",
       "      <td>19.293478</td>\n",
       "      <td>224.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>883.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.327887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.076779</td>\n",
       "      <td>25.103896</td>\n",
       "      <td>13.605794</td>\n",
       "      <td>15.483876</td>\n",
       "      <td>25.103896</td>\n",
       "      <td>263.0</td>\n",
       "      <td>985.0</td>\n",
       "      <td>979.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.481117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>112</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>113</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-0.105263</td>\n",
       "      <td>0.082251</td>\n",
       "      <td>26.088235</td>\n",
       "      <td>13.979707</td>\n",
       "      <td>16.027177</td>\n",
       "      <td>26.088235</td>\n",
       "      <td>124.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.536640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>114</td>\n",
       "      <td>https://insights.blackcoffer.com/estimating-th...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102455</td>\n",
       "      <td>24.971014</td>\n",
       "      <td>12.362159</td>\n",
       "      <td>14.933269</td>\n",
       "      <td>24.971014</td>\n",
       "      <td>213.0</td>\n",
       "      <td>876.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.403947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>115</td>\n",
       "      <td>https://insights.blackcoffer.com/covid-19-how-...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>27.245902</td>\n",
       "      <td>15.643803</td>\n",
       "      <td>17.155882</td>\n",
       "      <td>27.245902</td>\n",
       "      <td>260.0</td>\n",
       "      <td>934.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.619134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>116</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120192</td>\n",
       "      <td>83.181818</td>\n",
       "      <td>6.448087</td>\n",
       "      <td>35.851962</td>\n",
       "      <td>83.181818</td>\n",
       "      <td>59.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.108197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0       37  https://insights.blackcoffer.com/ai-in-healthc...            68.0   \n",
       "1       38  https://insights.blackcoffer.com/what-if-the-c...            60.0   \n",
       "2       39  https://insights.blackcoffer.com/what-jobs-wil...            65.0   \n",
       "3       40  https://insights.blackcoffer.com/will-machine-...            64.0   \n",
       "4       41  https://insights.blackcoffer.com/will-ai-repla...            59.0   \n",
       "..     ...                                                ...             ...   \n",
       "75     112  https://insights.blackcoffer.com/how-will-covi...             0.0   \n",
       "76     113  https://insights.blackcoffer.com/coronavirus-i...            17.0   \n",
       "77     114  https://insights.blackcoffer.com/estimating-th...            48.0   \n",
       "78     115  https://insights.blackcoffer.com/covid-19-how-...            21.0   \n",
       "79     116  https://insights.blackcoffer.com/how-will-covi...            25.0   \n",
       "\n",
       "    NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0             34.0        0.333333            0.081535            26.640000   \n",
       "1             38.0        0.224490            0.115023            20.487500   \n",
       "2             38.0        0.262136            0.093297            22.470588   \n",
       "3             27.0        0.406593            0.101336            19.293478   \n",
       "4             23.0        0.439024            0.076779            25.103896   \n",
       "..             ...             ...                 ...                  ...   \n",
       "75             0.0        0.000000            0.000000             0.000000   \n",
       "76            21.0       -0.105263            0.082251            26.088235   \n",
       "77            48.0        0.000000            0.102455            24.971014   \n",
       "78            39.0       -0.300000            0.060241            27.245902   \n",
       "79            25.0        0.000000            0.120192            83.181818   \n",
       "\n",
       "    PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                     18.068068  17.883227                         26.640000   \n",
       "1                     10.982306  12.587923                         20.487500   \n",
       "2                     17.591623  16.024885                         22.470588   \n",
       "3                     12.619718  12.765279                         19.293478   \n",
       "4                     13.605794  15.483876                         25.103896   \n",
       "..                          ...        ...                               ...   \n",
       "75                     0.000000   0.000000                          0.000000   \n",
       "76                    13.979707  16.027177                         26.088235   \n",
       "77                    12.362159  14.933269                         24.971014   \n",
       "78                    15.643803  17.155882                         27.245902   \n",
       "79                     6.448087  35.851962                         83.181818   \n",
       "\n",
       "    COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                361.0      1153.0             1163.0                1.0   \n",
       "1                180.0       771.0              758.0                6.0   \n",
       "2                336.0      1001.0              995.0                2.0   \n",
       "3                224.0       887.0              883.0               17.0   \n",
       "4                263.0       985.0              979.0               12.0   \n",
       "..                 ...         ...                ...                ...   \n",
       "75                 0.0         0.0                0.0                0.0   \n",
       "76               124.0       460.0              453.0                7.0   \n",
       "77               213.0       876.0              867.0                2.0   \n",
       "78               260.0       934.0              943.0                2.0   \n",
       "79                59.0       477.0              481.0               42.0   \n",
       "\n",
       "    AVG WORD LENGTH  \n",
       "0          5.065065  \n",
       "1          4.176937  \n",
       "2          4.795288  \n",
       "3          4.327887  \n",
       "4          4.481117  \n",
       "..              ...  \n",
       "75         0.000000  \n",
       "76         4.536640  \n",
       "77         4.403947  \n",
       "78         4.619134  \n",
       "79         4.108197  \n",
       "\n",
       "[80 rows x 15 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Output Data Structure.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
